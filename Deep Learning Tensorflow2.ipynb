{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luchinan/GitHub-Tutorial/blob/master/Deep%20Learning%20Tensorflow2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vyvnyl61ddc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UDJYYXQo1j_I"
      },
      "source": [
        "# Deep Reinforcement Learning with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PoeQUyRo1j_R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2e775493-8fff-41d5-c1b3-d79486344e47",
        "id": "8zHaPv6F1j_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/be/e4e2cc0b4896648fe6d5e45dda6d8c3b784823301708cfe4ff96de9e01cf/tf_nightly_2.0_preview-2.0.0.dev20191002-cp36-cp36m-manylinux2010_x86_64.whl (95.2MB)\n",
            "\u001b[K     |████████████████████████████████| 95.2MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.16.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Collecting tb-nightly<2.2.0a0,>=2.1.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/3b/1479cccf0bdcdabc0e046eb761fdf413125cc1b5e83398e15ff00000f6ed/tb_nightly-2.1.0a20191010-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/f5/790508e193121ab301cb40cada7f451c531404051ac9249f21b1f5484450/tensorflow_estimator_2.0_preview-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
            "Installing collected packages: tb-nightly, tensorflow-estimator-2.0-preview, tf-nightly-2.0-preview\n",
            "Successfully installed tb-nightly-2.1.0a20191010 tensorflow-estimator-2.0-preview-2.0.0 tf-nightly-2.0-preview-2.0.0.dev20191002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "is5B-EWZ1j_q",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "import tensorflow.keras.losses as kls\n",
        "import tensorflow.keras.optimizers as ko"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NTWtu4vm1j_8",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "01ac4705-f0ca-4712-8b2b-7fc0e042c50d",
        "id": "xGQR3SEk1kAT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"TensorFlow Ver: \", tf.__version__)\n",
        "print(\"Eager Execution:\", tf.executing_eagerly())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Ver:  2.0.0-dev20191002\n",
            "Eager Execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b08e8ca-93e1-4299-fa91-875958300cea",
        "id": "NZyAisAX1kAl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# eager by default!\n",
        "print(\"1 + 2 + 3 + 4 + 5 =\", tf.reduce_sum([1, 2, 3, 4, 5]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 + 2 + 3 + 4 + 5 = tf.Tensor(15, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yFQVViv41kA1"
      },
      "source": [
        "## Advantage Actor-Critic with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z_2hotzw1kA6"
      },
      "source": [
        "### Policy & Value Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKI6-5mN1kBC",
        "colab": {}
      },
      "source": [
        "class ProbabilityDistribution(tf.keras.Model):\n",
        "    def call(self, logits):\n",
        "        # sample a random categorical action from given logits\n",
        "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__('mlp_policy')\n",
        "        # no tf.get_variable(), just simple Keras API\n",
        "        self.hidden1 = kl.Dense(128, activation='relu')\n",
        "        self.hidden2 = kl.Dense(128, activation='relu')\n",
        "        self.value = kl.Dense(1, name='value')\n",
        "        # logits are unnormalized log probabilities\n",
        "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
        "        self.dist = ProbabilityDistribution()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs is a numpy array, convert to Tensor\n",
        "        x = tf.convert_to_tensor(inputs)\n",
        "        # separate hidden layers from the same input tensor\n",
        "        hidden_logs = self.hidden1(x)\n",
        "        hidden_vals = self.hidden2(x)\n",
        "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
        "\n",
        "    def action_value(self, obs):\n",
        "        # executes call() under the hood\n",
        "        logits, value = self.predict(obs)\n",
        "        action = self.dist.predict(logits)\n",
        "        # a simpler option, will become clear later why we don't use it\n",
        "        # action = tf.random.categorical(logits, 1)\n",
        "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QPADAZyg1kBO"
      },
      "source": [
        "### Advantage Actor-Critic Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49dTagCm1kBV",
        "colab": {}
      },
      "source": [
        "class A2CAgent:\n",
        "    def __init__(self, model):\n",
        "        # hyperparameters for loss terms, gamma is the discount coefficient\n",
        "        self.params = {\n",
        "            'gamma': 0.99,\n",
        "            'value': 0.5,\n",
        "            'entropy': 0.0001\n",
        "        }\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            optimizer=ko.RMSprop(lr=0.0007),\n",
        "            # define separate losses for policy logits and value estimate\n",
        "            loss=[self._logits_loss, self._value_loss]\n",
        "        )\n",
        "    \n",
        "    def train(self, env, batch_sz=32, updates=1000):\n",
        "        # storage helpers for a single batch of data\n",
        "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
        "        rewards, dones, values = np.empty((3, batch_sz))\n",
        "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
        "        # training loop: collect samples, send to optimizer, repeat updates times\n",
        "        ep_rews = [0.0]\n",
        "        next_obs = env.reset()\n",
        "        for update in range(updates):\n",
        "            for step in range(batch_sz):\n",
        "                observations[step] = next_obs.copy()\n",
        "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
        "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
        "\n",
        "                ep_rews[-1] += rewards[step]\n",
        "                if dones[step]:\n",
        "                    ep_rews.append(0.0)\n",
        "                    next_obs = env.reset()\n",
        "                    logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rews)-1, ep_rews[-2]))\n",
        "\n",
        "            _, next_value = self.model.action_value(next_obs[None, :])\n",
        "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
        "            # a trick to input actions and advantages through same API\n",
        "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
        "            # performs a full training step on the collected batch\n",
        "            # note: no need to mess around with gradients, Keras API handles it\n",
        "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
        "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n",
        "        return ep_rews\n",
        "\n",
        "    def test(self, env, render=False):\n",
        "        obs, done, ep_reward = env.reset(), False, 0\n",
        "        while not done:\n",
        "            action, _ = self.model.action_value(obs[None, :])\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            if render:\n",
        "                env.render()\n",
        "        return ep_reward\n",
        "\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
        "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "        # returns are calculated as discounted sum of future rewards\n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
        "        returns = returns[:-1]\n",
        "        # advantages are returns - baseline, value estimates in our case\n",
        "        advantages = returns - values\n",
        "        return returns, advantages\n",
        "    \n",
        "    def _value_loss(self, returns, value):\n",
        "        # value loss is typically MSE between value estimates and returns\n",
        "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
        "\n",
        "    def _logits_loss(self, acts_and_advs, logits):\n",
        "        # a trick to input actions and advantages through same API\n",
        "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
        "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
        "        # from_logits argument ensures transformation into normalized probabilities\n",
        "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        # policy loss is defined by policy gradients, weighted by advantages\n",
        "        # note: we only calculate the loss on the actions we've actually taken\n",
        "        actions = tf.cast(actions, tf.int32)\n",
        "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
        "        # entropy loss can be calculated via CE over itself\n",
        "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
        "        # here signs are flipped because optimizer minimizes\n",
        "        return policy_loss - self.params['entropy']*entropy_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7236a043-9eff-4b54-d101-c4f93466542a",
        "id": "0vzgVeit1kBj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "model.action_value(env.reset()[None, :])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(0), array([0.00545876], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lhv05ZN11kBz"
      },
      "source": [
        "## Training A2C Agent & Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4GIfPqv1kB7",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "agent = A2CAgent(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eUAenJhr1kCN"
      },
      "source": [
        "### Testing with Random Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bfe9cfaa-c677-4b2e-8341-c572b02be3cc",
        "id": "OKDlCk_M1kCV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rewards_sum = agent.test(env)\n",
        "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Episode Reward: 12 out of 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c4f61c96-ce68-4316-a545-31adfa7a37ba",
        "id": "LphiNR6t1kCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "rewards_history = agent.train(env)\n",
        "print(\"Finished training.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Episode: 001, Reward: 016\n",
            "INFO:root:Episode: 002, Reward: 018\n",
            "INFO:root:Episode: 003, Reward: 045\n",
            "INFO:root:Episode: 004, Reward: 013\n",
            "INFO:root:Episode: 005, Reward: 020\n",
            "INFO:root:Episode: 006, Reward: 022\n",
            "INFO:root:Episode: 007, Reward: 017\n",
            "INFO:root:Episode: 008, Reward: 015\n",
            "INFO:root:Episode: 009, Reward: 012\n",
            "INFO:root:Episode: 010, Reward: 018\n",
            "INFO:root:Episode: 011, Reward: 013\n",
            "INFO:root:Episode: 012, Reward: 023\n",
            "INFO:root:Episode: 013, Reward: 019\n",
            "INFO:root:Episode: 014, Reward: 040\n",
            "INFO:root:Episode: 015, Reward: 013\n",
            "INFO:root:Episode: 016, Reward: 016\n",
            "INFO:root:Episode: 017, Reward: 024\n",
            "INFO:root:Episode: 018, Reward: 014\n",
            "INFO:root:Episode: 019, Reward: 016\n",
            "INFO:root:Episode: 020, Reward: 029\n",
            "INFO:root:Episode: 021, Reward: 011\n",
            "INFO:root:Episode: 022, Reward: 039\n",
            "INFO:root:Episode: 023, Reward: 024\n",
            "INFO:root:Episode: 024, Reward: 034\n",
            "INFO:root:Episode: 025, Reward: 037\n",
            "INFO:root:Episode: 026, Reward: 017\n",
            "INFO:root:Episode: 027, Reward: 023\n",
            "INFO:root:Episode: 028, Reward: 058\n",
            "INFO:root:Episode: 029, Reward: 014\n",
            "INFO:root:Episode: 030, Reward: 017\n",
            "INFO:root:Episode: 031, Reward: 027\n",
            "INFO:root:Episode: 032, Reward: 010\n",
            "INFO:root:Episode: 033, Reward: 038\n",
            "INFO:root:Episode: 034, Reward: 015\n",
            "INFO:root:Episode: 035, Reward: 017\n",
            "INFO:root:Episode: 036, Reward: 017\n",
            "INFO:root:Episode: 037, Reward: 072\n",
            "INFO:root:Episode: 038, Reward: 015\n",
            "INFO:root:Episode: 039, Reward: 025\n",
            "INFO:root:Episode: 040, Reward: 049\n",
            "INFO:root:Episode: 041, Reward: 038\n",
            "INFO:root:Episode: 042, Reward: 014\n",
            "INFO:root:Episode: 043, Reward: 016\n",
            "INFO:root:Episode: 044, Reward: 065\n",
            "INFO:root:Episode: 045, Reward: 064\n",
            "INFO:root:Episode: 046, Reward: 020\n",
            "INFO:root:Episode: 047, Reward: 013\n",
            "INFO:root:Episode: 048, Reward: 018\n",
            "INFO:root:Episode: 049, Reward: 032\n",
            "INFO:root:Episode: 050, Reward: 026\n",
            "INFO:root:Episode: 051, Reward: 022\n",
            "INFO:root:Episode: 052, Reward: 020\n",
            "INFO:root:Episode: 053, Reward: 022\n",
            "INFO:root:Episode: 054, Reward: 013\n",
            "INFO:root:Episode: 055, Reward: 022\n",
            "INFO:root:Episode: 056, Reward: 042\n",
            "INFO:root:Episode: 057, Reward: 036\n",
            "INFO:root:Episode: 058, Reward: 025\n",
            "INFO:root:Episode: 059, Reward: 027\n",
            "INFO:root:Episode: 060, Reward: 032\n",
            "INFO:root:Episode: 061, Reward: 032\n",
            "INFO:root:Episode: 062, Reward: 068\n",
            "INFO:root:Episode: 063, Reward: 010\n",
            "INFO:root:Episode: 064, Reward: 018\n",
            "INFO:root:Episode: 065, Reward: 049\n",
            "INFO:root:Episode: 066, Reward: 039\n",
            "INFO:root:Episode: 067, Reward: 035\n",
            "INFO:root:Episode: 068, Reward: 025\n",
            "INFO:root:Episode: 069, Reward: 023\n",
            "INFO:root:Episode: 070, Reward: 038\n",
            "INFO:root:Episode: 071, Reward: 012\n",
            "INFO:root:Episode: 072, Reward: 046\n",
            "INFO:root:Episode: 073, Reward: 042\n",
            "INFO:root:Episode: 074, Reward: 044\n",
            "INFO:root:Episode: 075, Reward: 027\n",
            "INFO:root:Episode: 076, Reward: 012\n",
            "INFO:root:Episode: 077, Reward: 042\n",
            "INFO:root:Episode: 078, Reward: 033\n",
            "INFO:root:Episode: 079, Reward: 085\n",
            "INFO:root:Episode: 080, Reward: 022\n",
            "INFO:root:Episode: 081, Reward: 066\n",
            "INFO:root:Episode: 082, Reward: 040\n",
            "INFO:root:Episode: 083, Reward: 014\n",
            "INFO:root:Episode: 084, Reward: 028\n",
            "INFO:root:Episode: 085, Reward: 065\n",
            "INFO:root:Episode: 086, Reward: 033\n",
            "INFO:root:Episode: 087, Reward: 043\n",
            "INFO:root:Episode: 088, Reward: 025\n",
            "INFO:root:Episode: 089, Reward: 011\n",
            "INFO:root:Episode: 090, Reward: 061\n",
            "INFO:root:Episode: 091, Reward: 037\n",
            "INFO:root:Episode: 092, Reward: 015\n",
            "INFO:root:Episode: 093, Reward: 038\n",
            "INFO:root:Episode: 094, Reward: 100\n",
            "INFO:root:Episode: 095, Reward: 011\n",
            "INFO:root:Episode: 096, Reward: 014\n",
            "INFO:root:Episode: 097, Reward: 019\n",
            "INFO:root:Episode: 098, Reward: 066\n",
            "INFO:root:Episode: 099, Reward: 031\n",
            "INFO:root:Episode: 100, Reward: 058\n",
            "INFO:root:Episode: 101, Reward: 028\n",
            "INFO:root:Episode: 102, Reward: 016\n",
            "INFO:root:Episode: 103, Reward: 038\n",
            "INFO:root:Episode: 104, Reward: 030\n",
            "INFO:root:Episode: 105, Reward: 033\n",
            "INFO:root:Episode: 106, Reward: 036\n",
            "INFO:root:Episode: 107, Reward: 070\n",
            "INFO:root:Episode: 108, Reward: 089\n",
            "INFO:root:Episode: 109, Reward: 017\n",
            "INFO:root:Episode: 110, Reward: 015\n",
            "INFO:root:Episode: 111, Reward: 027\n",
            "INFO:root:Episode: 112, Reward: 048\n",
            "INFO:root:Episode: 113, Reward: 029\n",
            "INFO:root:Episode: 114, Reward: 067\n",
            "INFO:root:Episode: 115, Reward: 074\n",
            "INFO:root:Episode: 116, Reward: 062\n",
            "INFO:root:Episode: 117, Reward: 035\n",
            "INFO:root:Episode: 118, Reward: 024\n",
            "INFO:root:Episode: 119, Reward: 063\n",
            "INFO:root:Episode: 120, Reward: 031\n",
            "INFO:root:Episode: 121, Reward: 045\n",
            "INFO:root:Episode: 122, Reward: 016\n",
            "INFO:root:Episode: 123, Reward: 067\n",
            "INFO:root:Episode: 124, Reward: 080\n",
            "INFO:root:Episode: 125, Reward: 039\n",
            "INFO:root:Episode: 126, Reward: 015\n",
            "INFO:root:Episode: 127, Reward: 014\n",
            "INFO:root:Episode: 128, Reward: 043\n",
            "INFO:root:Episode: 129, Reward: 050\n",
            "INFO:root:Episode: 130, Reward: 015\n",
            "INFO:root:Episode: 131, Reward: 039\n",
            "INFO:root:Episode: 132, Reward: 047\n",
            "INFO:root:Episode: 133, Reward: 054\n",
            "INFO:root:Episode: 134, Reward: 042\n",
            "INFO:root:Episode: 135, Reward: 031\n",
            "INFO:root:Episode: 136, Reward: 019\n",
            "INFO:root:Episode: 137, Reward: 068\n",
            "INFO:root:Episode: 138, Reward: 069\n",
            "INFO:root:Episode: 139, Reward: 055\n",
            "INFO:root:Episode: 140, Reward: 072\n",
            "INFO:root:Episode: 141, Reward: 092\n",
            "INFO:root:Episode: 142, Reward: 090\n",
            "INFO:root:Episode: 143, Reward: 017\n",
            "INFO:root:Episode: 144, Reward: 071\n",
            "INFO:root:Episode: 145, Reward: 089\n",
            "INFO:root:Episode: 146, Reward: 058\n",
            "INFO:root:Episode: 147, Reward: 049\n",
            "INFO:root:Episode: 148, Reward: 037\n",
            "INFO:root:Episode: 149, Reward: 012\n",
            "INFO:root:Episode: 150, Reward: 058\n",
            "INFO:root:Episode: 151, Reward: 024\n",
            "INFO:root:Episode: 152, Reward: 099\n",
            "INFO:root:Episode: 153, Reward: 025\n",
            "INFO:root:Episode: 154, Reward: 038\n",
            "INFO:root:Episode: 155, Reward: 035\n",
            "INFO:root:Episode: 156, Reward: 043\n",
            "INFO:root:Episode: 157, Reward: 032\n",
            "INFO:root:Episode: 158, Reward: 018\n",
            "INFO:root:Episode: 159, Reward: 079\n",
            "INFO:root:Episode: 160, Reward: 051\n",
            "INFO:root:Episode: 161, Reward: 056\n",
            "INFO:root:Episode: 162, Reward: 079\n",
            "INFO:root:Episode: 163, Reward: 102\n",
            "INFO:root:Episode: 164, Reward: 029\n",
            "INFO:root:Episode: 165, Reward: 160\n",
            "INFO:root:Episode: 166, Reward: 055\n",
            "INFO:root:Episode: 167, Reward: 041\n",
            "INFO:root:Episode: 168, Reward: 026\n",
            "INFO:root:Episode: 169, Reward: 134\n",
            "INFO:root:Episode: 170, Reward: 056\n",
            "INFO:root:Episode: 171, Reward: 029\n",
            "INFO:root:Episode: 172, Reward: 072\n",
            "INFO:root:Episode: 173, Reward: 050\n",
            "INFO:root:Episode: 174, Reward: 076\n",
            "INFO:root:Episode: 175, Reward: 034\n",
            "INFO:root:Episode: 176, Reward: 037\n",
            "INFO:root:Episode: 177, Reward: 034\n",
            "INFO:root:Episode: 178, Reward: 026\n",
            "INFO:root:Episode: 179, Reward: 020\n",
            "INFO:root:Episode: 180, Reward: 036\n",
            "INFO:root:Episode: 181, Reward: 039\n",
            "INFO:root:Episode: 182, Reward: 031\n",
            "INFO:root:Episode: 183, Reward: 035\n",
            "INFO:root:Episode: 184, Reward: 082\n",
            "INFO:root:Episode: 185, Reward: 026\n",
            "INFO:root:Episode: 186, Reward: 030\n",
            "INFO:root:Episode: 187, Reward: 034\n",
            "INFO:root:Episode: 188, Reward: 018\n",
            "INFO:root:Episode: 189, Reward: 038\n",
            "INFO:root:Episode: 190, Reward: 045\n",
            "INFO:root:Episode: 191, Reward: 028\n",
            "INFO:root:Episode: 192, Reward: 039\n",
            "INFO:root:Episode: 193, Reward: 046\n",
            "INFO:root:Episode: 194, Reward: 058\n",
            "INFO:root:Episode: 195, Reward: 098\n",
            "INFO:root:Episode: 196, Reward: 041\n",
            "INFO:root:Episode: 197, Reward: 060\n",
            "INFO:root:Episode: 198, Reward: 055\n",
            "INFO:root:Episode: 199, Reward: 047\n",
            "INFO:root:Episode: 200, Reward: 066\n",
            "INFO:root:Episode: 201, Reward: 141\n",
            "INFO:root:Episode: 202, Reward: 086\n",
            "INFO:root:Episode: 203, Reward: 032\n",
            "INFO:root:Episode: 204, Reward: 048\n",
            "INFO:root:Episode: 205, Reward: 024\n",
            "INFO:root:Episode: 206, Reward: 042\n",
            "INFO:root:Episode: 207, Reward: 089\n",
            "INFO:root:Episode: 208, Reward: 064\n",
            "INFO:root:Episode: 209, Reward: 012\n",
            "INFO:root:Episode: 210, Reward: 029\n",
            "INFO:root:Episode: 211, Reward: 033\n",
            "INFO:root:Episode: 212, Reward: 095\n",
            "INFO:root:Episode: 213, Reward: 032\n",
            "INFO:root:Episode: 214, Reward: 040\n",
            "INFO:root:Episode: 215, Reward: 051\n",
            "INFO:root:Episode: 216, Reward: 048\n",
            "INFO:root:Episode: 217, Reward: 055\n",
            "INFO:root:Episode: 218, Reward: 024\n",
            "INFO:root:Episode: 219, Reward: 059\n",
            "INFO:root:Episode: 220, Reward: 040\n",
            "INFO:root:Episode: 221, Reward: 039\n",
            "INFO:root:Episode: 222, Reward: 122\n",
            "INFO:root:Episode: 223, Reward: 028\n",
            "INFO:root:Episode: 224, Reward: 077\n",
            "INFO:root:Episode: 225, Reward: 093\n",
            "INFO:root:Episode: 226, Reward: 092\n",
            "INFO:root:Episode: 227, Reward: 155\n",
            "INFO:root:Episode: 228, Reward: 042\n",
            "INFO:root:Episode: 229, Reward: 043\n",
            "INFO:root:Episode: 230, Reward: 053\n",
            "INFO:root:Episode: 231, Reward: 043\n",
            "INFO:root:Episode: 232, Reward: 071\n",
            "INFO:root:Episode: 233, Reward: 043\n",
            "INFO:root:Episode: 234, Reward: 038\n",
            "INFO:root:Episode: 235, Reward: 089\n",
            "INFO:root:Episode: 236, Reward: 035\n",
            "INFO:root:Episode: 237, Reward: 099\n",
            "INFO:root:Episode: 238, Reward: 041\n",
            "INFO:root:Episode: 239, Reward: 031\n",
            "INFO:root:Episode: 240, Reward: 051\n",
            "INFO:root:Episode: 241, Reward: 096\n",
            "INFO:root:Episode: 242, Reward: 063\n",
            "INFO:root:Episode: 243, Reward: 050\n",
            "INFO:root:Episode: 244, Reward: 064\n",
            "INFO:root:Episode: 245, Reward: 037\n",
            "INFO:root:Episode: 246, Reward: 032\n",
            "INFO:root:Episode: 247, Reward: 154\n",
            "INFO:root:Episode: 248, Reward: 038\n",
            "INFO:root:Episode: 249, Reward: 084\n",
            "INFO:root:Episode: 250, Reward: 056\n",
            "INFO:root:Episode: 251, Reward: 081\n",
            "INFO:root:Episode: 252, Reward: 104\n",
            "INFO:root:Episode: 253, Reward: 038\n",
            "INFO:root:Episode: 254, Reward: 066\n",
            "INFO:root:Episode: 255, Reward: 113\n",
            "INFO:root:Episode: 256, Reward: 042\n",
            "INFO:root:Episode: 257, Reward: 063\n",
            "INFO:root:Episode: 258, Reward: 033\n",
            "INFO:root:Episode: 259, Reward: 056\n",
            "INFO:root:Episode: 260, Reward: 090\n",
            "INFO:root:Episode: 261, Reward: 053\n",
            "INFO:root:Episode: 262, Reward: 034\n",
            "INFO:root:Episode: 263, Reward: 045\n",
            "INFO:root:Episode: 264, Reward: 034\n",
            "INFO:root:Episode: 265, Reward: 089\n",
            "INFO:root:Episode: 266, Reward: 050\n",
            "INFO:root:Episode: 267, Reward: 037\n",
            "INFO:root:Episode: 268, Reward: 038\n",
            "INFO:root:Episode: 269, Reward: 038\n",
            "INFO:root:Episode: 270, Reward: 059\n",
            "INFO:root:Episode: 271, Reward: 033\n",
            "INFO:root:Episode: 272, Reward: 038\n",
            "INFO:root:Episode: 273, Reward: 049\n",
            "INFO:root:Episode: 274, Reward: 038\n",
            "INFO:root:Episode: 275, Reward: 052\n",
            "INFO:root:Episode: 276, Reward: 082\n",
            "INFO:root:Episode: 277, Reward: 054\n",
            "INFO:root:Episode: 278, Reward: 067\n",
            "INFO:root:Episode: 279, Reward: 026\n",
            "INFO:root:Episode: 280, Reward: 063\n",
            "INFO:root:Episode: 281, Reward: 112\n",
            "INFO:root:Episode: 282, Reward: 083\n",
            "INFO:root:Episode: 283, Reward: 117\n",
            "INFO:root:Episode: 284, Reward: 097\n",
            "INFO:root:Episode: 285, Reward: 053\n",
            "INFO:root:Episode: 286, Reward: 068\n",
            "INFO:root:Episode: 287, Reward: 107\n",
            "INFO:root:Episode: 288, Reward: 064\n",
            "INFO:root:Episode: 289, Reward: 017\n",
            "INFO:root:Episode: 290, Reward: 116\n",
            "INFO:root:Episode: 291, Reward: 032\n",
            "INFO:root:Episode: 292, Reward: 060\n",
            "INFO:root:Episode: 293, Reward: 082\n",
            "INFO:root:Episode: 294, Reward: 066\n",
            "INFO:root:Episode: 295, Reward: 056\n",
            "INFO:root:Episode: 296, Reward: 099\n",
            "INFO:root:Episode: 297, Reward: 098\n",
            "INFO:root:Episode: 298, Reward: 075\n",
            "INFO:root:Episode: 299, Reward: 029\n",
            "INFO:root:Episode: 300, Reward: 076\n",
            "INFO:root:Episode: 301, Reward: 200\n",
            "INFO:root:Episode: 302, Reward: 075\n",
            "INFO:root:Episode: 303, Reward: 133\n",
            "INFO:root:Episode: 304, Reward: 140\n",
            "INFO:root:Episode: 305, Reward: 042\n",
            "INFO:root:Episode: 306, Reward: 097\n",
            "INFO:root:Episode: 307, Reward: 094\n",
            "INFO:root:Episode: 308, Reward: 046\n",
            "INFO:root:Episode: 309, Reward: 115\n",
            "INFO:root:Episode: 310, Reward: 066\n",
            "INFO:root:Episode: 311, Reward: 116\n",
            "INFO:root:Episode: 312, Reward: 079\n",
            "INFO:root:Episode: 313, Reward: 200\n",
            "INFO:root:Episode: 314, Reward: 133\n",
            "INFO:root:Episode: 315, Reward: 113\n",
            "INFO:root:Episode: 316, Reward: 200\n",
            "INFO:root:Episode: 317, Reward: 200\n",
            "INFO:root:Episode: 318, Reward: 148\n",
            "INFO:root:Episode: 319, Reward: 058\n",
            "INFO:root:Episode: 320, Reward: 159\n",
            "INFO:root:Episode: 321, Reward: 177\n",
            "INFO:root:Episode: 322, Reward: 118\n",
            "INFO:root:Episode: 323, Reward: 039\n",
            "INFO:root:Episode: 324, Reward: 086\n",
            "INFO:root:Episode: 325, Reward: 200\n",
            "INFO:root:Episode: 326, Reward: 085\n",
            "INFO:root:Episode: 327, Reward: 125\n",
            "INFO:root:Episode: 328, Reward: 163\n",
            "INFO:root:Episode: 329, Reward: 107\n",
            "INFO:root:Episode: 330, Reward: 140\n",
            "INFO:root:Episode: 331, Reward: 119\n",
            "INFO:root:Episode: 332, Reward: 200\n",
            "INFO:root:Episode: 333, Reward: 200\n",
            "INFO:root:Episode: 334, Reward: 116\n",
            "INFO:root:Episode: 335, Reward: 142\n",
            "INFO:root:Episode: 336, Reward: 068\n",
            "INFO:root:Episode: 337, Reward: 152\n",
            "INFO:root:Episode: 338, Reward: 200\n",
            "INFO:root:Episode: 339, Reward: 200\n",
            "INFO:root:Episode: 340, Reward: 188\n",
            "INFO:root:Episode: 341, Reward: 158\n",
            "INFO:root:Episode: 342, Reward: 200\n",
            "INFO:root:Episode: 343, Reward: 200\n",
            "INFO:root:Episode: 344, Reward: 142\n",
            "INFO:root:Episode: 345, Reward: 200\n",
            "INFO:root:Episode: 346, Reward: 200\n",
            "INFO:root:Episode: 347, Reward: 200\n",
            "INFO:root:Episode: 348, Reward: 200\n",
            "INFO:root:Episode: 349, Reward: 125\n",
            "INFO:root:Episode: 350, Reward: 087\n",
            "INFO:root:Episode: 351, Reward: 162\n",
            "INFO:root:Episode: 352, Reward: 177\n",
            "INFO:root:Episode: 353, Reward: 143\n",
            "INFO:root:Episode: 354, Reward: 188\n",
            "INFO:root:Episode: 355, Reward: 102\n",
            "INFO:root:Episode: 356, Reward: 200\n",
            "INFO:root:Episode: 357, Reward: 158\n",
            "INFO:root:Episode: 358, Reward: 116\n",
            "INFO:root:Episode: 359, Reward: 154\n",
            "INFO:root:Episode: 360, Reward: 178\n",
            "INFO:root:Episode: 361, Reward: 200\n",
            "INFO:root:Episode: 362, Reward: 129\n",
            "INFO:root:Episode: 363, Reward: 118\n",
            "INFO:root:Episode: 364, Reward: 181\n",
            "INFO:root:Episode: 365, Reward: 083\n",
            "INFO:root:Episode: 366, Reward: 118\n",
            "INFO:root:Episode: 367, Reward: 191\n",
            "INFO:root:Episode: 368, Reward: 175\n",
            "INFO:root:Episode: 369, Reward: 174\n",
            "INFO:root:Episode: 370, Reward: 181\n",
            "INFO:root:Episode: 371, Reward: 200\n",
            "INFO:root:Episode: 372, Reward: 040\n",
            "INFO:root:Episode: 373, Reward: 041\n",
            "INFO:root:Episode: 374, Reward: 076\n",
            "INFO:root:Episode: 375, Reward: 190\n",
            "INFO:root:Episode: 376, Reward: 200\n",
            "INFO:root:Episode: 377, Reward: 071\n",
            "INFO:root:Episode: 378, Reward: 120\n",
            "INFO:root:Episode: 379, Reward: 128\n",
            "INFO:root:Episode: 380, Reward: 132\n",
            "INFO:root:Episode: 381, Reward: 136\n",
            "INFO:root:Episode: 382, Reward: 136\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H1e8xSE31kC0"
      },
      "source": [
        "### Testing with Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7pb4EKBJ1kC4",
        "colab": {}
      },
      "source": [
        "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Om4Z11t81kDG"
      },
      "source": [
        "### Training Rewards History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RIA1DTkO1kDK",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gOOebJfD1kDX"
      },
      "source": [
        "## Static Computational Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OIbyRcPi1kDc",
        "colab": {}
      },
      "source": [
        "with tf.Graph().as_default():\n",
        "    print(\"Eager Execution:\", tf.executing_eagerly()) # False\n",
        "\n",
        "    model = Model(num_actions=env.action_space.n)\n",
        "    agent = A2CAgent(model)\n",
        "\n",
        "    rewards_history = agent.train(env)\n",
        "    print(\"Finished training, testing...\")\n",
        "    print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xo2BygkD1kDw"
      },
      "source": [
        "## Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N5j8TxLA1kD1",
        "colab": {}
      },
      "source": [
        "# Note: comparing wall time isn't exactly fair due to specifics of how things are executed on multi-core CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NWKnEEpL1kEB",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "obs = np.repeat(env.reset()[None, :], 100000, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "giVBRhpB1kEO"
      },
      "source": [
        "### Eager Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EhUCUoPJ1kET",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Model(env.action_space.n)\n",
        "model.run_eagerly = True\n",
        "\n",
        "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "_ = model(obs)\n",
        "# _ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13uwb8Wd1kEq"
      },
      "source": [
        "### Static Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYj_HfYu1kEu",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    model = Model(env.action_space.n)\n",
        "\n",
        "    print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "    print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "    _ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kK6rtjQn1kE8"
      },
      "source": [
        "### Default Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cbb-kYcc1kFA",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Model(env.action_space.n)\n",
        "\n",
        "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "_ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPMkxVFBg85m",
        "colab_type": "text"
      },
      "source": [
        "# Deep Reinforcement Learning with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IFjMkr_g4ZK",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08aIvFN7hNhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QipBW2W7g4ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "import tensorflow.keras.losses as kls\n",
        "import tensorflow.keras.optimizers as ko"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyx5ah4Xg4ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmjplMPwg4ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"TensorFlow Ver: \", tf.__version__)\n",
        "print(\"Eager Execution:\", tf.executing_eagerly())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr62U1IPg4ZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eager by default!\n",
        "print(\"1 + 2 + 3 + 4 + 5 =\", tf.reduce_sum([1, 2, 3, 4, 5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kO-Mnllg4ZX",
        "colab_type": "text"
      },
      "source": [
        "## Advantage Actor-Critic with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFGPAkQNg4ZX",
        "colab_type": "text"
      },
      "source": [
        "### Policy & Value Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQ8YHmxg4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProbabilityDistribution(tf.keras.Model):\n",
        "    def call(self, logits):\n",
        "        # sample a random categorical action from given logits\n",
        "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__('mlp_policy')\n",
        "        # no tf.get_variable(), just simple Keras API\n",
        "        self.hidden1 = kl.Dense(128, activation='relu')\n",
        "        self.hidden2 = kl.Dense(128, activation='relu')\n",
        "        self.value = kl.Dense(1, name='value')\n",
        "        # logits are unnormalized log probabilities\n",
        "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
        "        self.dist = ProbabilityDistribution()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs is a numpy array, convert to Tensor\n",
        "        x = tf.convert_to_tensor(inputs)\n",
        "        # separate hidden layers from the same input tensor\n",
        "        hidden_logs = self.hidden1(x)\n",
        "        hidden_vals = self.hidden2(x)\n",
        "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
        "\n",
        "    def action_value(self, obs):\n",
        "        # executes call() under the hood\n",
        "        logits, value = self.predict(obs)\n",
        "        action = self.dist.predict(logits)\n",
        "        # a simpler option, will become clear later why we don't use it\n",
        "        # action = tf.random.categorical(logits, 1)\n",
        "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g8dFW1rg4Zb",
        "colab_type": "text"
      },
      "source": [
        "### Advantage Actor-Critic Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-pKLzu2g4Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A2CAgent:\n",
        "    def __init__(self, model):\n",
        "        # hyperparameters for loss terms, gamma is the discount coefficient\n",
        "        self.params = {\n",
        "            'gamma': 0.99,\n",
        "            'value': 0.5,\n",
        "            'entropy': 0.0001\n",
        "        }\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            optimizer=ko.RMSprop(lr=0.0007),\n",
        "            # define separate losses for policy logits and value estimate\n",
        "            loss=[self._logits_loss, self._value_loss]\n",
        "        )\n",
        "    \n",
        "    def train(self, env, batch_sz=32, updates=1000):\n",
        "        # storage helpers for a single batch of data\n",
        "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
        "        rewards, dones, values = np.empty((3, batch_sz))\n",
        "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
        "        # training loop: collect samples, send to optimizer, repeat updates times\n",
        "        ep_rews = [0.0]\n",
        "        next_obs = env.reset()\n",
        "        for update in range(updates):\n",
        "            for step in range(batch_sz):\n",
        "                observations[step] = next_obs.copy()\n",
        "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
        "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
        "\n",
        "                ep_rews[-1] += rewards[step]\n",
        "                if dones[step]:\n",
        "                    ep_rews.append(0.0)\n",
        "                    next_obs = env.reset()\n",
        "                    logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rews)-1, ep_rews[-2]))\n",
        "\n",
        "            _, next_value = self.model.action_value(next_obs[None, :])\n",
        "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
        "            # a trick to input actions and advantages through same API\n",
        "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
        "            # performs a full training step on the collected batch\n",
        "            # note: no need to mess around with gradients, Keras API handles it\n",
        "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
        "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n",
        "        return ep_rews\n",
        "\n",
        "    def test(self, env, render=False):\n",
        "        obs, done, ep_reward = env.reset(), False, 0\n",
        "        while not done:\n",
        "            action, _ = self.model.action_value(obs[None, :])\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            if render:\n",
        "                env.render()\n",
        "        return ep_reward\n",
        "\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
        "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
        "        # returns are calculated as discounted sum of future rewards\n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
        "        returns = returns[:-1]\n",
        "        # advantages are returns - baseline, value estimates in our case\n",
        "        advantages = returns - values\n",
        "        return returns, advantages\n",
        "    \n",
        "    def _value_loss(self, returns, value):\n",
        "        # value loss is typically MSE between value estimates and returns\n",
        "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
        "\n",
        "    def _logits_loss(self, acts_and_advs, logits):\n",
        "        # a trick to input actions and advantages through same API\n",
        "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
        "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
        "        # from_logits argument ensures transformation into normalized probabilities\n",
        "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        # policy loss is defined by policy gradients, weighted by advantages\n",
        "        # note: we only calculate the loss on the actions we've actually taken\n",
        "        actions = tf.cast(actions, tf.int32)\n",
        "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
        "        # entropy loss can be calculated via CE over itself\n",
        "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
        "        # here signs are flipped because optimizer minimizes\n",
        "        return policy_loss - self.params['entropy']*entropy_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf1I04Ttg4Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "model.action_value(env.reset()[None, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LFt3keg4Zh",
        "colab_type": "text"
      },
      "source": [
        "## Training A2C Agent & Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mxzrB1Ng4Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "model = Model(num_actions=env.action_space.n)\n",
        "agent = A2CAgent(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR7v0mBig4Zl",
        "colab_type": "text"
      },
      "source": [
        "### Testing with Random Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3JFA1pDg4Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards_sum = agent.test(env)\n",
        "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FRn0aRg4Zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "rewards_history = agent.train(env)\n",
        "print(\"Finished training.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ7Ss140g4Zs",
        "colab_type": "text"
      },
      "source": [
        "### Testing with Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbM_P6CMg4Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RFDN0Smg4Zv",
        "colab_type": "text"
      },
      "source": [
        "### Training Rewards History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou6PDK8ig4Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrtyfS8Sg4Z0",
        "colab_type": "text"
      },
      "source": [
        "## Static Computational Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WA6tTTrg4Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Graph().as_default():\n",
        "    print(\"Eager Execution:\", tf.executing_eagerly()) # False\n",
        "\n",
        "    model = Model(num_actions=env.action_space.n)\n",
        "    agent = A2CAgent(model)\n",
        "\n",
        "    rewards_history = agent.train(env)\n",
        "    print(\"Finished training, testing...\")\n",
        "    print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k55LpUOg4Z4",
        "colab_type": "text"
      },
      "source": [
        "## Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR5J2WCZg4Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: comparing wall time isn't exactly fair due to specifics of how things are executed on multi-core CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl51m1PXg4Z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "obs = np.repeat(env.reset()[None, :], 100000, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y6LxDvBg4Z9",
        "colab_type": "text"
      },
      "source": [
        "### Eager Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CpTEYnxg4Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Model(env.action_space.n)\n",
        "model.run_eagerly = True\n",
        "\n",
        "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "_ = model(obs)\n",
        "# _ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-9FMyk-g4aB",
        "colab_type": "text"
      },
      "source": [
        "### Static Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3RqVlslg4aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    model = Model(env.action_space.n)\n",
        "\n",
        "    print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "    print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "    _ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXKB5vcsg4aG",
        "colab_type": "text"
      },
      "source": [
        "### Default Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aqkh1yTg4aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Model(env.action_space.n)\n",
        "\n",
        "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
        "print(\"Eager Keras Model:\", model.run_eagerly)\n",
        "\n",
        "_ = model.predict(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}